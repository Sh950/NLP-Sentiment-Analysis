{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8273f8a-a6a4-4ff5-87f8-6d132baefb3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projects/dharel/shimonsh/.local/lib/python3.8/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/projects/dharel/shimonsh/.local/lib/python3.8/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/projects/dharel/shimonsh/.local/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA A40\n",
      "CPU cores (logical): 96\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np           # Numerical arrays & math\n",
    "import pandas as pd          # Tabular data (DataFrames)\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import html \n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import config\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CPU cores (logical):\", os.cpu_count())\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c624b11",
   "metadata": {},
   "source": [
    "The function converts words into numerical embeddings and supports random - embedding, Pre-Traind - GloVe, or Word2Vec (w2v)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59168954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, itos, vocab_size=0, embedding_dim=300, embedding_type=\"glove\"):\n",
    "    \"\"\"\n",
    "    Build an embedding matrix.\n",
    "\n",
    "    Args:\n",
    "        vocab: vocabulary object\n",
    "        itos : index to string\n",
    "        vocab_size (int): size of the vocabulary\n",
    "        embedding_dim (int): embedding vector size\n",
    "        embedding_type (str): \"glove\", \"w2v\", or \"random\"\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: embedding matrix of shape (vocab_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    embedding_matrix = torch.randn(vocab_size, embedding_dim) * 0.6\n",
    "    \n",
    "    if embedding_type == \"glove\":\n",
    "        glove = GloVe(name=\"6B\", dim=300, cache=\"./glove_cache\")\n",
    "        for i, word in enumerate(itos):\n",
    "            if word in glove.stoi:\n",
    "                embedding_matrix[i] = glove[word]\n",
    "\n",
    "    elif embedding_type == \"w2v\":\n",
    "        w2v_model = Word2Vec.load(str(config.W2V_MODEL_PATH))\n",
    "        for i, word in enumerate(itos):\n",
    "            if word in w2v_model.wv.key_to_index:\n",
    "                embedding_matrix[i] = torch.tensor(w2v_model.wv[word], dtype=torch.float32)\n",
    "                \n",
    "    elif embedding_type == \"random\":\n",
    "        pass  # already random\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3990e621-4844-4553-a7af-0b54f016bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based text classification model.\n",
    "    Architecture:\n",
    "     Embedding -> Embedding Dropout -> LSTM -> LayerNorm -> Dropout -> Fully Connected (Linear) -> Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,\n",
    "                 pad_idx, embedding_matrix, num_layers=1, bidirectional=True, dropout=0.2, embedding_dropout=0.2, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            embed_dim (int): Dimension of word embeddings\n",
    "            hidden_dim (int): LSTM hidden size\n",
    "            num_classes (int): Number of output classes\n",
    "            pad_idx (int): Index of the <pad> token\n",
    "            embedding_matrix (torch.Tensor): Pre-trained embeddings\n",
    "            num_layers (int): Number of LSTM layers\n",
    "            bidirectional (bool): Use bidirectional LSTM\n",
    "            dropout (float): Dropout probability\n",
    "            embedding_dropout (float): Embedding dropout probability\n",
    "            freeze_embeddings (bool): Freeze embedding weights \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer with padding_idx\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings, padding_idx=pad_idx)\n",
    "        \n",
    "             \n",
    "         # Embedding dropout\n",
    "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        lstm_out_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # LayerNorm on the hidden states\n",
    "        self.layer_norm = nn.LayerNorm(lstm_out_dim)\n",
    "\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            x (torch.LongTensor): Input tensor of shape [batch_size, seq_len]\n",
    "        Returns:\n",
    "            logits (torch.FloatTensor): Output tensor of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # x -> [batch_size, seq_len] -> embedding -> [batch_size, seq_len, embed_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embedding_dropout(embedded) \n",
    "\n",
    "        # LSTM forward\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "\n",
    "        # Max pooling\n",
    "        pooled, _ = torch.max(lstm_out, dim=1)\n",
    "        \n",
    "        # Average Pooling \n",
    "        # pooled = torch.mean(lstm_out, dim=1)  \n",
    "\n",
    "        last_hidden = pooled   \n",
    "        last_hidden = self.layer_norm(last_hidden)\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected layer -> logits\n",
    "        logits = self.fc(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2baf71b-62c2-4eaa-9711-510cd61a1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    Args:\n",
    "        model (nn.Module): The LSTM model\n",
    "        dataloader (DataLoader): Training data loader\n",
    "        optimizer (torch.optim.Optimizer): Optimizer\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): CPU or GPU\n",
    "    Returns:\n",
    "        avg_loss (float): Average loss over the epoch\n",
    "        avg_acc (float): Average accuracy over the epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()          # Clear previous gradients\n",
    "        logits = model(X_batch)        # Forward pass\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()                # Backpropagation\n",
    "        optimizer.step()               # Update weights\n",
    "\n",
    "        total_loss += loss.item() * X_batch.size(0)  # Sum loss over batch\n",
    "        preds = logits.argmax(dim=1)                # Predicted classes\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y_batch.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b606b23e-a700-4864-8f8a-f4d508cd11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation or test set.\n",
    "    Args:\n",
    "        model (nn.Module): The LSTM model\n",
    "        dataloader (DataLoader): Validation or test loader\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): CPU or GPU\n",
    "    Returns:\n",
    "        avg_loss (float): Average loss\n",
    "        avg_acc (float): Average accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y_batch.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    \n",
    "    # Compute precision and recall for class 1\n",
    "    precision_1 = precision_score(all_labels, all_preds, pos_label=1)\n",
    "    recall_1 = recall_score(all_labels, all_preds, pos_label=1)\n",
    "    \n",
    "    return avg_loss, avg_acc, precision_1, recall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a660b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts_and_labels(df):\n",
    "    texts = df['text'].tolist()\n",
    "    labels = (df['label'] == 1).astype(int).tolist()\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels, labels_tensor\n",
    "\n",
    "def word_tokenizer(texts, vocab_size):\n",
    "\n",
    "    def tokenizer(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        token_counts.update(tokens)\n",
    "         \n",
    "    most_common = token_counts.most_common(vocab_size - 2)\n",
    "    vocab = {\n",
    "        '<pad>': 0,\n",
    "        '<unk>': 1\n",
    "    }\n",
    "\n",
    "    for idx, (token, _) in enumerate(most_common, start=2):\n",
    "        vocab[token] = idx\n",
    "\n",
    "    itos = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "    return vocab, tokenizer, itos\n",
    "  \n",
    "def text_to_indices(text, vocab, tokenizer):\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokenizer(text)]  \n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "def pad_sequence_with_maxlen(sequences, max_len, padding_value=0, dtype=torch.long, batch_first=True):\n",
    "    \"\"\"\n",
    "    Pads or truncates a list of 1D tensors to a fixed maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of torch.Tensor): List of 1D tensors (variable-length sequences).\n",
    "        max_len (int): Desired maximum sequence length.\n",
    "        padding_value (int, optional): Value used for padding shorter sequences. Default is 0.\n",
    "        dtype (torch.dtype, optional): Data type of the output tensor. Default is torch.long.\n",
    "        batch_first (bool, optional): If True, output shape is [batch_size, max_len].\n",
    "                                      If False, output shape is [max_len, batch_size].\n",
    "                                      Default is True.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor where all sequences are either truncated or padded to max_len.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for seq in sequences:\n",
    "        seq = seq[:max_len]  # truncate if longer than max_len\n",
    "        if len(seq) < max_len:\n",
    "            pad_size = max_len - len(seq)\n",
    "            seq = torch.cat([seq, torch.full((pad_size,), padding_value, dtype=dtype)])\n",
    "        processed.append(seq)\n",
    "\n",
    "    result = torch.stack(processed)\n",
    "\n",
    "    if not batch_first:\n",
    "        result = result.transpose(0, 1)  # shape: [max_len, batch_size]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d947f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_pipeline(df_train, df_test, vocab_size, max_len, batch):\n",
    "    \"\"\"\n",
    "    Prepares train/val/test loaders from a DataFrame with tokenized_clean_text.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Must contain 'tokenized_clean_text' and 'label'\n",
    "        vocab_size (int): Max vocabulary size\n",
    "        max_len (int): Max sequence length for padding\n",
    "        batch (int): Batch size\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, vocab, tokenizer, itos\n",
    "    \"\"\"\n",
    "    # --- Split test data to val and test---\n",
    "    df_val, df_test = train_test_split(\n",
    "       df_test, test_size=0.5, random_state=42, stratify=df_test['label'])\n",
    "    \n",
    "    \n",
    "    x_train, x_val, x_test = df_train['text'], df_val['text'], df_test['text']\n",
    "    y_train, y_val, y_test = df_train['label'], df_val['label'], df_test['label']\n",
    "  \n",
    "\n",
    "    # --- Convert to list of lists ---\n",
    "    train_texts = list(x_train)\n",
    "    val_texts   = list(x_val)\n",
    "    test_texts  = list(x_test)\n",
    "\n",
    "    # --- Build vocabulary ---\n",
    "    counter = Counter(word for tokens in train_texts for word in tokens)\n",
    "    most_common = counter.most_common(vocab_size-2)  # reserve <pad> & <unk>\n",
    "    itos = [\"<pad>\", \"<unk>\"] + [w for w, _ in most_common]\n",
    "    stoi = {w: i for i, w in enumerate(itos)}\n",
    "    vocab = stoi\n",
    "\n",
    "    # --- Text to indices (vectorized) ---\n",
    "    def tokens_to_indices(tokens):\n",
    "        return [vocab.get(w, vocab[\"<unk>\"]) for w in tokens]\n",
    "\n",
    "    train_sequences = [tokens_to_indices(tokens) for tokens in train_texts]\n",
    "    val_sequences   = [tokens_to_indices(tokens) for tokens in val_texts]\n",
    "    test_sequences  = [tokens_to_indices(tokens) for tokens in test_texts]\n",
    "\n",
    "    # --- Padding using PyTorch ---\n",
    "    PAD_INDEX = vocab[\"<pad>\"]\n",
    "\n",
    "    def pad_sequences(sequences, max_len):\n",
    "        return torch.tensor([\n",
    "            seq[:max_len] + [PAD_INDEX]*(max_len - len(seq)) if len(seq)<max_len else seq[:max_len]\n",
    "            for seq in sequences\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "    train_sequences = pad_sequences(train_sequences, max_len)\n",
    "    val_sequences   = pad_sequences(val_sequences, max_len)\n",
    "    test_sequences  = pad_sequences(test_sequences, max_len)\n",
    "\n",
    "    # --- Labels to tensor ---\n",
    "    y_train_tensor = torch.tensor(list(y_train), dtype=torch.long)\n",
    "    y_val_tensor   = torch.tensor(list(y_val), dtype=torch.long)\n",
    "    y_test_tensor  = torch.tensor(list(y_test), dtype=torch.long)\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_sequences, y_train_tensor), batch_size=batch, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(val_sequences, y_val_tensor), batch_size=batch)\n",
    "    test_loader  = DataLoader(TensorDataset(test_sequences, y_test_tensor), batch_size=batch)\n",
    "\n",
    "    print(\"Data pipeline preparation completed.\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, vocab, stoi, itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17df2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_pipeline(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    vocab_size=5000,\n",
    "    max_len=64,\n",
    "    embedding_type='glove',  \n",
    "    embedding_dim=100,       \n",
    "    batch_size=64,\n",
    "    num_layers=1,\n",
    "    hidden_dim=128,\n",
    "    dropout=0.2,\n",
    "    embedding_dropout=0.1,\n",
    "    freeze_embeddings=True\n",
    "):\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    save_path = os.path.join(\"models\", f\"best_lstm_{embedding_type}_model.pt\")\n",
    "    \n",
    "    # Loaders\n",
    "    train_loader, val_loader, test_loader, vocab, tokenizer, itos = prepare_data_pipeline(\n",
    "        df_train = df_train,\n",
    "        df_test = df_test,\n",
    "        vocab_size = vocab_size,\n",
    "        max_len = max_len,\n",
    "        batch = batch_size)\n",
    "    \n",
    "    # Embedding\n",
    "    embedding_matrix = build_embedding_matrix(\n",
    "    vocab=vocab,\n",
    "    itos=itos,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    embedding_type=embedding_type)\n",
    "    \n",
    "    # Create the model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=2,\n",
    "        pad_idx=vocab[\"<pad>\"],\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        embedding_dropout = embedding_dropout,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        freeze_embeddings = freeze_embeddings\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "    \n",
    "    # Training and Evaluation Loops\n",
    "    EPOCHS = 15\n",
    "    best_val_acc = 0.0\n",
    "    patience, counter = 5, 0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, val_prec, val_rec = evaluate_func(model, val_loader, criterion, device)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        if val_acc > best_val_acc:  # improvement\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:                       # no improvement\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "              f\"Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f} | \"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "        #scheduler.step()\n",
    "\n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load (save_path))\n",
    "    test_loss, test_acc, test_prec, test_rec = evaluate_func(model, test_loader, criterion, device)\n",
    "    print(\"\\nbest lstm with \" + embedding_type + \" embedding result on test:\")\n",
    "    print({test_acc:.4})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05dda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_DIM = 256 \n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2126e6e",
   "metadata": {},
   "source": [
    "Train & Evaluate\n",
    "\n",
    "IMDB\n",
    "\n",
    "Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63621813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Embeddings Parameters\n",
    "df_train = pd.read_csv(config.IMDB_TRAIN_PATH)\n",
    "df_test = pd.read_csv(config.IMDB_TEST_PATH)\n",
    "vocab_size = 8000\n",
    "max_len = 256\n",
    "batch_size = 32\n",
    "embedding_type = 'random' # 'glove', 'w2v' or 'random'\n",
    "embedding_dim = 256  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbcbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.6361, Train Acc: 0.6246 | Val Loss: 0.5502, Val Acc: 0.7060 | Val Precision: 0.6709, Val Recall: 0.8088 | Time: 35.11s\n",
      "Epoch 2/15 | Train Loss: 0.5467, Train Acc: 0.7131 | Val Loss: 0.5403, Val Acc: 0.7230 | Val Precision: 0.6705, Val Recall: 0.8768 | Time: 33.44s\n",
      "Epoch 3/15 | Train Loss: 0.5105, Train Acc: 0.7399 | Val Loss: 0.5229, Val Acc: 0.7348 | Val Precision: 0.8051, Val Recall: 0.6196 | Time: 33.37s\n",
      "Epoch 4/15 | Train Loss: 0.4793, Train Acc: 0.7641 | Val Loss: 0.4923, Val Acc: 0.7578 | Val Precision: 0.7181, Val Recall: 0.8488 | Time: 33.35s\n",
      "Epoch 5/15 | Train Loss: 0.4500, Train Acc: 0.7832 | Val Loss: 0.4696, Val Acc: 0.7698 | Val Precision: 0.7501, Val Recall: 0.8092 | Time: 33.35s\n",
      "Epoch 6/15 | Train Loss: 0.4267, Train Acc: 0.7963 | Val Loss: 0.5055, Val Acc: 0.7580 | Val Precision: 0.7163, Val Recall: 0.8544 | Time: 33.39s\n",
      "Epoch 7/15 | Train Loss: 0.4062, Train Acc: 0.8106 | Val Loss: 0.5089, Val Acc: 0.7588 | Val Precision: 0.8235, Val Recall: 0.6588 | Time: 33.37s\n",
      "Epoch 8/15 | Train Loss: 0.3811, Train Acc: 0.8258 | Val Loss: 0.4852, Val Acc: 0.7752 | Val Precision: 0.7592, Val Recall: 0.8060 | Time: 33.31s\n",
      "Epoch 9/15 | Train Loss: 0.3605, Train Acc: 0.8363 | Val Loss: 0.4847, Val Acc: 0.7750 | Val Precision: 0.8017, Val Recall: 0.7308 | Time: 33.28s\n",
      "Epoch 10/15 | Train Loss: 0.3351, Train Acc: 0.8507 | Val Loss: 0.5025, Val Acc: 0.7724 | Val Precision: 0.7854, Val Recall: 0.7496 | Time: 33.23s\n",
      "Epoch 11/15 | Train Loss: 0.3120, Train Acc: 0.8611 | Val Loss: 0.5018, Val Acc: 0.7732 | Val Precision: 0.7813, Val Recall: 0.7588 | Time: 33.38s\n",
      "Epoch 12/15 | Train Loss: 0.2940, Train Acc: 0.8689 | Val Loss: 0.5384, Val Acc: 0.7680 | Val Precision: 0.7873, Val Recall: 0.7344 | Time: 33.33s\n",
      "Epoch 13/15 | Train Loss: 0.2699, Train Acc: 0.8826 | Val Loss: 0.5243, Val Acc: 0.7784 | Val Precision: 0.7827, Val Recall: 0.7708 | Time: 33.33s\n",
      "Epoch 14/15 | Train Loss: 0.2481, Train Acc: 0.8943 | Val Loss: 0.5592, Val Acc: 0.7766 | Val Precision: 0.7769, Val Recall: 0.7760 | Time: 33.35s\n",
      "Epoch 15/15 | Train Loss: 0.2263, Train Acc: 0.9042 | Val Loss: 0.6085, Val Acc: 0.7740 | Val Precision: 0.7597, Val Recall: 0.8016 | Time: 33.36s\n",
      "\n",
      "best lstm with random embedding result on test:\n",
      "{0.777: 0.4}\n"
     ]
    }
   ],
   "source": [
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7ca7e",
   "metadata": {},
   "source": [
    "GLOVE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa86b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.7174, Train Acc: 0.5041 | Val Loss: 0.6936, Val Acc: 0.5110 | Val Precision: 0.5064, Val Recall: 0.8740 | Time: 32.96s\n",
      "Epoch 2/15 | Train Loss: 0.7017, Train Acc: 0.5101 | Val Loss: 0.6874, Val Acc: 0.5524 | Val Precision: 0.5626, Val Recall: 0.4708 | Time: 32.84s\n",
      "Epoch 3/15 | Train Loss: 0.6904, Train Acc: 0.5413 | Val Loss: 0.6844, Val Acc: 0.5430 | Val Precision: 0.5259, Val Recall: 0.8728 | Time: 32.86s\n",
      "Epoch 4/15 | Train Loss: 0.6674, Train Acc: 0.5887 | Val Loss: 0.6331, Val Acc: 0.6406 | Val Precision: 0.6231, Val Recall: 0.7116 | Time: 32.88s\n",
      "Epoch 5/15 | Train Loss: 0.5962, Train Acc: 0.6686 | Val Loss: 0.5617, Val Acc: 0.6924 | Val Precision: 0.6610, Val Recall: 0.7900 | Time: 32.85s\n",
      "Epoch 6/15 | Train Loss: 0.5422, Train Acc: 0.7152 | Val Loss: 0.5307, Val Acc: 0.7234 | Val Precision: 0.7606, Val Recall: 0.6520 | Time: 32.82s\n",
      "Epoch 7/15 | Train Loss: 0.5084, Train Acc: 0.7414 | Val Loss: 0.5025, Val Acc: 0.7450 | Val Precision: 0.7226, Val Recall: 0.7952 | Time: 32.76s\n",
      "Epoch 8/15 | Train Loss: 0.4806, Train Acc: 0.7582 | Val Loss: 0.5012, Val Acc: 0.7528 | Val Precision: 0.7341, Val Recall: 0.7928 | Time: 32.76s\n",
      "Epoch 9/15 | Train Loss: 0.4591, Train Acc: 0.7755 | Val Loss: 0.4973, Val Acc: 0.7510 | Val Precision: 0.7261, Val Recall: 0.8060 | Time: 32.75s\n",
      "Epoch 10/15 | Train Loss: 0.4403, Train Acc: 0.7873 | Val Loss: 0.4937, Val Acc: 0.7584 | Val Precision: 0.7409, Val Recall: 0.7948 | Time: 32.86s\n",
      "Epoch 11/15 | Train Loss: 0.4230, Train Acc: 0.7981 | Val Loss: 0.4888, Val Acc: 0.7534 | Val Precision: 0.7840, Val Recall: 0.6996 | Time: 32.84s\n",
      "Epoch 12/15 | Train Loss: 0.4076, Train Acc: 0.8096 | Val Loss: 0.4835, Val Acc: 0.7638 | Val Precision: 0.7567, Val Recall: 0.7776 | Time: 32.88s\n",
      "Epoch 13/15 | Train Loss: 0.3915, Train Acc: 0.8169 | Val Loss: 0.4742, Val Acc: 0.7698 | Val Precision: 0.7743, Val Recall: 0.7616 | Time: 32.81s\n",
      "Epoch 14/15 | Train Loss: 0.3728, Train Acc: 0.8278 | Val Loss: 0.4833, Val Acc: 0.7640 | Val Precision: 0.7855, Val Recall: 0.7264 | Time: 32.73s\n",
      "Epoch 15/15 | Train Loss: 0.3616, Train Acc: 0.8351 | Val Loss: 0.5055, Val Acc: 0.7638 | Val Precision: 0.7297, Val Recall: 0.8380 | Time: 32.71s\n",
      "\n",
      "best lstm with glove embedding result on test:\n",
      "{0.7706: 0.4}\n"
     ]
    }
   ],
   "source": [
    "embedding_type = 'glove'\n",
    "embedding_dim = 300  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = True\n",
    "\n",
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a5581",
   "metadata": {},
   "source": [
    "W2V Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af98bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.6822, Train Acc: 0.5571 | Val Loss: 0.6106, Val Acc: 0.6448 | Val Precision: 0.6381, Val Recall: 0.6692 | Time: 33.33s\n",
      "Epoch 2/15 | Train Loss: 0.5817, Train Acc: 0.6781 | Val Loss: 0.5541, Val Acc: 0.6974 | Val Precision: 0.7506, Val Recall: 0.5912 | Time: 33.35s\n",
      "Epoch 3/15 | Train Loss: 0.5393, Train Acc: 0.7141 | Val Loss: 0.5310, Val Acc: 0.7254 | Val Precision: 0.7605, Val Recall: 0.6580 | Time: 33.35s\n",
      "Epoch 4/15 | Train Loss: 0.5068, Train Acc: 0.7422 | Val Loss: 0.5031, Val Acc: 0.7532 | Val Precision: 0.7390, Val Recall: 0.7828 | Time: 33.36s\n",
      "Epoch 5/15 | Train Loss: 0.4793, Train Acc: 0.7621 | Val Loss: 0.5288, Val Acc: 0.7216 | Val Precision: 0.8354, Val Recall: 0.5520 | Time: 33.35s\n",
      "Epoch 6/15 | Train Loss: 0.4556, Train Acc: 0.7775 | Val Loss: 0.4744, Val Acc: 0.7676 | Val Precision: 0.7374, Val Recall: 0.8312 | Time: 33.39s\n",
      "Epoch 7/15 | Train Loss: 0.4288, Train Acc: 0.7961 | Val Loss: 0.4702, Val Acc: 0.7710 | Val Precision: 0.7746, Val Recall: 0.7644 | Time: 33.38s\n",
      "Epoch 8/15 | Train Loss: 0.4010, Train Acc: 0.8113 | Val Loss: 0.4640, Val Acc: 0.7762 | Val Precision: 0.7741, Val Recall: 0.7800 | Time: 33.34s\n",
      "Epoch 9/15 | Train Loss: 0.3717, Train Acc: 0.8290 | Val Loss: 0.4786, Val Acc: 0.7758 | Val Precision: 0.7659, Val Recall: 0.7944 | Time: 33.32s\n",
      "Epoch 10/15 | Train Loss: 0.3381, Train Acc: 0.8487 | Val Loss: 0.4965, Val Acc: 0.7798 | Val Precision: 0.7906, Val Recall: 0.7612 | Time: 33.33s\n",
      "Epoch 11/15 | Train Loss: 0.3068, Train Acc: 0.8658 | Val Loss: 0.5090, Val Acc: 0.7774 | Val Precision: 0.7962, Val Recall: 0.7456 | Time: 33.39s\n",
      "Epoch 12/15 | Train Loss: 0.2695, Train Acc: 0.8837 | Val Loss: 0.5509, Val Acc: 0.7726 | Val Precision: 0.7929, Val Recall: 0.7380 | Time: 33.25s\n",
      "Epoch 13/15 | Train Loss: 0.2392, Train Acc: 0.8994 | Val Loss: 0.6058, Val Acc: 0.7624 | Val Precision: 0.7903, Val Recall: 0.7144 | Time: 33.30s\n",
      "Epoch 14/15 | Train Loss: 0.2089, Train Acc: 0.9130 | Val Loss: 0.6567, Val Acc: 0.7736 | Val Precision: 0.7691, Val Recall: 0.7820 | Time: 33.21s\n",
      "Early stopping at epoch 15\n",
      "\n",
      "best lstm with w2v embedding result on test:\n",
      "{0.7774: 0.4}\n"
     ]
    }
   ],
   "source": [
    "embedding_type = 'w2v'\n",
    "embedding_dim = 256  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = False\n",
    "\n",
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd89054",
   "metadata": {},
   "source": [
    "Rotten Tomatoes \n",
    "\n",
    "Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f9f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.7384, Train Acc: 0.5199 | Val Loss: 0.7627, Val Acc: 0.5000 | Val Precision: 0.5000, Val Recall: 1.0000 | Time: 3.88s\n",
      "Epoch 2/15 | Train Loss: 0.6848, Train Acc: 0.5635 | Val Loss: 0.6676, Val Acc: 0.5807 | Val Precision: 0.7108, Val Recall: 0.2720 | Time: 3.86s\n",
      "Epoch 3/15 | Train Loss: 0.6582, Train Acc: 0.6104 | Val Loss: 0.6115, Val Acc: 0.6614 | Val Precision: 0.6599, Val Recall: 0.6660 | Time: 3.86s\n",
      "Epoch 4/15 | Train Loss: 0.6217, Train Acc: 0.6590 | Val Loss: 0.5929, Val Acc: 0.6923 | Val Precision: 0.6881, Val Recall: 0.7036 | Time: 3.86s\n",
      "Epoch 5/15 | Train Loss: 0.5768, Train Acc: 0.6936 | Val Loss: 0.6460, Val Acc: 0.6454 | Val Precision: 0.8112, Val Recall: 0.3790 | Time: 3.87s\n",
      "Epoch 6/15 | Train Loss: 0.5261, Train Acc: 0.7306 | Val Loss: 0.5642, Val Acc: 0.7336 | Val Precision: 0.7666, Val Recall: 0.6717 | Time: 3.87s\n",
      "Epoch 7/15 | Train Loss: 0.4848, Train Acc: 0.7633 | Val Loss: 0.5694, Val Acc: 0.7148 | Val Precision: 0.7034, Val Recall: 0.7430 | Time: 3.87s\n",
      "Epoch 8/15 | Train Loss: 0.4027, Train Acc: 0.8131 | Val Loss: 0.6006, Val Acc: 0.7233 | Val Precision: 0.7038, Val Recall: 0.7711 | Time: 3.87s\n",
      "Epoch 9/15 | Train Loss: 0.3380, Train Acc: 0.8540 | Val Loss: 0.6351, Val Acc: 0.7345 | Val Precision: 0.7480, Val Recall: 0.7073 | Time: 3.88s\n",
      "Epoch 10/15 | Train Loss: 0.2678, Train Acc: 0.8893 | Val Loss: 0.7029, Val Acc: 0.7345 | Val Precision: 0.7637, Val Recall: 0.6792 | Time: 3.87s\n",
      "Epoch 11/15 | Train Loss: 0.2225, Train Acc: 0.9082 | Val Loss: 0.7270, Val Acc: 0.7289 | Val Precision: 0.7302, Val Recall: 0.7261 | Time: 3.87s\n",
      "Epoch 12/15 | Train Loss: 0.1604, Train Acc: 0.9394 | Val Loss: 0.9472, Val Acc: 0.7129 | Val Precision: 0.7748, Val Recall: 0.6004 | Time: 3.87s\n",
      "Epoch 13/15 | Train Loss: 0.1306, Train Acc: 0.9497 | Val Loss: 0.9557, Val Acc: 0.7111 | Val Precision: 0.7111, Val Recall: 0.7111 | Time: 3.87s\n",
      "Early stopping at epoch 14\n",
      "\n",
      "best lstm with random embedding result on test:\n",
      "{0.7141518275538894: 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Data and Embeddings Parameters\n",
    "df_train = pd.read_csv(config.RT_TRAIN_PATH)\n",
    "df_test = pd.read_csv(config.RT_TEST_PATH)\n",
    "vocab_size = 5000\n",
    "max_len = 128\n",
    "batch_size = 32\n",
    "embedding_type = 'random' # 'glove', 'w2v' or 'random'\n",
    "embedding_dim = 256  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = False\n",
    "\n",
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fc0cf",
   "metadata": {},
   "source": [
    "GLOVE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51de3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.7372, Train Acc: 0.5165 | Val Loss: 0.7047, Val Acc: 0.5019 | Val Precision: 0.5009, Val Recall: 1.0000 | Time: 3.77s\n",
      "Epoch 2/15 | Train Loss: 0.6986, Train Acc: 0.5295 | Val Loss: 0.6707, Val Acc: 0.6004 | Val Precision: 0.5918, Val Recall: 0.6473 | Time: 3.78s\n",
      "Epoch 3/15 | Train Loss: 0.6893, Train Acc: 0.5498 | Val Loss: 0.6572, Val Acc: 0.6144 | Val Precision: 0.6282, Val Recall: 0.5610 | Time: 3.78s\n",
      "Epoch 4/15 | Train Loss: 0.6829, Train Acc: 0.5698 | Val Loss: 0.6460, Val Acc: 0.6341 | Val Precision: 0.6312, Val Recall: 0.6454 | Time: 3.78s\n",
      "Epoch 5/15 | Train Loss: 0.6589, Train Acc: 0.6129 | Val Loss: 0.6313, Val Acc: 0.6538 | Val Precision: 0.6323, Val Recall: 0.7355 | Time: 3.77s\n",
      "Epoch 6/15 | Train Loss: 0.6402, Train Acc: 0.6310 | Val Loss: 0.6460, Val Acc: 0.6295 | Val Precision: 0.7518, Val Recall: 0.3865 | Time: 3.77s\n",
      "Epoch 7/15 | Train Loss: 0.6007, Train Acc: 0.6772 | Val Loss: 0.6017, Val Acc: 0.6698 | Val Precision: 0.6399, Val Recall: 0.7767 | Time: 3.76s\n",
      "Epoch 8/15 | Train Loss: 0.5751, Train Acc: 0.7017 | Val Loss: 0.5962, Val Acc: 0.6914 | Val Precision: 0.6604, Val Recall: 0.7880 | Time: 3.76s\n",
      "Epoch 9/15 | Train Loss: 0.5405, Train Acc: 0.7289 | Val Loss: 0.6522, Val Acc: 0.6614 | Val Precision: 0.7792, Val Recall: 0.4503 | Time: 3.76s\n",
      "Epoch 10/15 | Train Loss: 0.5055, Train Acc: 0.7565 | Val Loss: 0.6604, Val Acc: 0.6604 | Val Precision: 0.7879, Val Recall: 0.4390 | Time: 3.76s\n",
      "Epoch 11/15 | Train Loss: 0.4668, Train Acc: 0.7838 | Val Loss: 0.5862, Val Acc: 0.6998 | Val Precision: 0.6760, Val Recall: 0.7674 | Time: 3.76s\n",
      "Epoch 12/15 | Train Loss: 0.4211, Train Acc: 0.8041 | Val Loss: 0.5941, Val Acc: 0.7101 | Val Precision: 0.7276, Val Recall: 0.6717 | Time: 3.77s\n",
      "Epoch 13/15 | Train Loss: 0.3798, Train Acc: 0.8273 | Val Loss: 0.6257, Val Acc: 0.7120 | Val Precision: 0.7025, Val Recall: 0.7355 | Time: 3.76s\n",
      "Epoch 14/15 | Train Loss: 0.3343, Train Acc: 0.8539 | Val Loss: 0.6875, Val Acc: 0.7158 | Val Precision: 0.7114, Val Recall: 0.7261 | Time: 3.76s\n",
      "Epoch 15/15 | Train Loss: 0.2795, Train Acc: 0.8796 | Val Loss: 0.7066, Val Acc: 0.7092 | Val Precision: 0.7299, Val Recall: 0.6642 | Time: 3.75s\n",
      "\n",
      "best lstm with glove embedding result on test:\n",
      "{0.7085285848172446: 0.4}\n"
     ]
    }
   ],
   "source": [
    "embedding_type = 'glove'\n",
    "embedding_dim = 300  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = True\n",
    "\n",
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39909cd6",
   "metadata": {},
   "source": [
    "W2V Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aabf83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline preparation completed.\n",
      "Epoch 1/15 | Train Loss: 0.7558, Train Acc: 0.4996 | Val Loss: 0.7077, Val Acc: 0.4775 | Val Precision: 0.4831, Val Recall: 0.6435 | Time: 3.87s\n",
      "Epoch 2/15 | Train Loss: 0.7053, Train Acc: 0.5140 | Val Loss: 0.6829, Val Acc: 0.5685 | Val Precision: 0.5720, Val Recall: 0.5441 | Time: 3.86s\n",
      "Epoch 3/15 | Train Loss: 0.7094, Train Acc: 0.5220 | Val Loss: 0.6985, Val Acc: 0.4850 | Val Precision: 0.4906, Val Recall: 0.7842 | Time: 3.86s\n",
      "Epoch 4/15 | Train Loss: 0.6966, Train Acc: 0.5321 | Val Loss: 0.6960, Val Acc: 0.5347 | Val Precision: 0.6637, Val Recall: 0.1407 | Time: 3.86s\n",
      "Epoch 5/15 | Train Loss: 0.6914, Train Acc: 0.5547 | Val Loss: 0.6717, Val Acc: 0.5807 | Val Precision: 0.5530, Val Recall: 0.8424 | Time: 3.85s\n",
      "Epoch 6/15 | Train Loss: 0.6834, Train Acc: 0.5641 | Val Loss: 0.6723, Val Acc: 0.5769 | Val Precision: 0.5610, Val Recall: 0.7073 | Time: 3.87s\n",
      "Epoch 7/15 | Train Loss: 0.6737, Train Acc: 0.5800 | Val Loss: 0.6487, Val Acc: 0.6191 | Val Precision: 0.6560, Val Recall: 0.5009 | Time: 3.87s\n",
      "Epoch 8/15 | Train Loss: 0.6580, Train Acc: 0.6089 | Val Loss: 0.6583, Val Acc: 0.6051 | Val Precision: 0.5749, Val Recall: 0.8068 | Time: 3.87s\n",
      "Epoch 9/15 | Train Loss: 0.6382, Train Acc: 0.6345 | Val Loss: 0.6388, Val Acc: 0.6370 | Val Precision: 0.6921, Val Recall: 0.4934 | Time: 3.86s\n",
      "Epoch 10/15 | Train Loss: 0.6111, Train Acc: 0.6644 | Val Loss: 0.6207, Val Acc: 0.6407 | Val Precision: 0.6198, Val Recall: 0.7280 | Time: 3.86s\n",
      "Epoch 11/15 | Train Loss: 0.5833, Train Acc: 0.6913 | Val Loss: 0.6103, Val Acc: 0.6595 | Val Precision: 0.6622, Val Recall: 0.6510 | Time: 3.88s\n",
      "Epoch 12/15 | Train Loss: 0.5426, Train Acc: 0.7241 | Val Loss: 0.6058, Val Acc: 0.6764 | Val Precision: 0.6822, Val Recall: 0.6604 | Time: 3.87s\n",
      "Epoch 13/15 | Train Loss: 0.5062, Train Acc: 0.7560 | Val Loss: 0.6145, Val Acc: 0.6961 | Val Precision: 0.7518, Val Recall: 0.5854 | Time: 3.86s\n",
      "Epoch 14/15 | Train Loss: 0.4296, Train Acc: 0.8027 | Val Loss: 0.6432, Val Acc: 0.6745 | Val Precision: 0.6643, Val Recall: 0.7054 | Time: 3.86s\n",
      "Epoch 15/15 | Train Loss: 0.3736, Train Acc: 0.8363 | Val Loss: 0.7366, Val Acc: 0.6726 | Val Precision: 0.6365, Val Recall: 0.8049 | Time: 3.86s\n",
      "\n",
      "best lstm with w2v embedding result on test:\n",
      "{0.6682286785379569: 0.4}\n"
     ]
    }
   ],
   "source": [
    "embedding_type = 'w2v'\n",
    "embedding_dim = 256  \n",
    "embedding_dropout = 0.1\n",
    "freeze_embedding = False\n",
    "\n",
    "run_lstm_pipeline(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    embedding_type=embedding_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_dropout=embedding_dropout,\n",
    "    freeze_embeddings=freeze_embedding\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
