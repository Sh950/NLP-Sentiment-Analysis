{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1589dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd   \n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89299f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Word2VecEmbedding (vector_size=256,window=5, min_count=3):  \n",
    "    \n",
    "    # --- Step 1: Preprocessing ---\n",
    "    all_texts = pd.concat([\n",
    "        pd.read_csv(config.IMDB_TRAIN_PATH),\n",
    "        pd.read_csv(config.IMDB_TEST_PATH),\n",
    "        pd.read_csv(config.RT_TRAIN_PATH),\n",
    "        pd.read_csv(config.RT_TEST_PATH),\n",
    "        pd.read_csv(config.IMDB_UNSUPERVISED_CLEAN_PATH),\n",
    "     ], ignore_index=True)\n",
    "\n",
    "    # --- Step 2: Tokenization ---\n",
    "    all_texts[\"text\"] = all_texts[\"text\"].astype(str)\n",
    "    tokenized_sentences = all_texts[\"text\"].str.split().tolist()\n",
    "\n",
    "\n",
    "    # --- Step 3:  Word2Vec ---\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized_sentences,\n",
    "        vector_size=vector_size,          # Dim of embedding\n",
    "        window=window,                    # Context window\n",
    "        min_count=min_count,              # Min frequency\n",
    "        workers=16,                       # CPU cores\n",
    "        sg=1,                     \n",
    "        negative=20,                      # Negative sampling\n",
    "        ns_exponent=0.75,         \n",
    "        epochs=7                  \n",
    ")\n",
    "\n",
    "    # --- Step 4: ---\n",
    "    print(\"Shape of embedding matrix:\", w2v_model.wv.vectors.shape)\n",
    "    print(\"Most similar to 'good':\", w2v_model.wv.most_similar('good'))\n",
    "\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b53aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (81968, 256)\n",
      "Most similar to 'good': [('decent', 0.7335376143455505), ('great', 0.6936540007591248), ('goodnot', 0.6720458269119263), ('goodso', 0.6608039140701294), ('bad', 0.6554890871047974), ('zseries', 0.6481224298477173), ('kirie', 0.6474978923797607), ('funbut', 0.6440195441246033), ('raws', 0.6436787843704224), ('allright', 0.6415371894836426)]\n"
     ]
    }
   ],
   "source": [
    "#Embedding On Corpus\n",
    "w2v_model = Word2VecEmbedding(vector_size=256, window=5, min_count=3)\n",
    "w2v_model.save(str(config.W2V_MODEL_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
